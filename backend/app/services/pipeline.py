"""
æ€»æ§ Pipeline æœåŠ¡
ä¸²è”è§£æã€åˆ†å—ã€å‘é‡åŒ–ã€æ£€ç´¢å’Œç”Ÿæˆæµç¨‹
"""
import logging
import time
from typing import List, Dict, Optional
from pydantic import BaseModel, Field
from pathlib import Path
import os
from tqdm import tqdm

from app.services.retrieval import RetrievalService
from app.services.llm import LLMService
from app.services.embedding import EmbeddingService
from app.services.pdf_to_markdown import PDFToMarkdownService
from app.services.chunking import DocumentChunker
from app.services.vector_db import VectorDBService
from app.utils.parser import DocumentParser
from app.storage.metadata import MetadataStorage
from app.utils.hash_utils import calculate_file_sha1
import json

logger = logging.getLogger(__name__)

class StructuredAnswer(BaseModel):
    """ç»“æ„åŒ–å›ç­”æ¨¡å‹"""
    answer: str = Field(description="é—®é¢˜çš„æœ€ç»ˆç­”æ¡ˆ")
    thoughts: str = Field(description="å›ç­”é—®é¢˜çš„æ¨ç†è¿‡ç¨‹æˆ–æ‘˜è¦")
    citations: List[int] = Field(description="å¼•ç”¨å†…å®¹çš„é¡µç åˆ—è¡¨")

class PipelinePaths:
    """Pipeline è·¯å¾„é…ç½®ç±»"""
    
    def __init__(
        self,
        base_dir: str = "./data",
        documents_dir: str = "documents",
        markdown_dir: str = "debug_data",
        chunked_reports_dir: str = "metadata/chunked_reports",
        vector_dbs_dir: str = "metadata/vector_dbs"
    ):
        """
        åˆå§‹åŒ–è·¯å¾„é…ç½®
        
        Args:
            base_dir: åŸºç¡€æ•°æ®ç›®å½•
            documents_dir: PDF æ–‡æ¡£ç›®å½•ï¼ˆç›¸å¯¹äº base_dirï¼‰
            markdown_dir: Markdown æ–‡ä»¶ç›®å½•ï¼ˆç›¸å¯¹äº base_dirï¼‰
            chunked_reports_dir: Chunk JSON æ–‡ä»¶ç›®å½•ï¼ˆç›¸å¯¹äº base_dirï¼‰
            vector_dbs_dir: FAISS ç´¢å¼•æ–‡ä»¶ç›®å½•ï¼ˆç›¸å¯¹äº base_dirï¼‰
        """
        self.base_dir = Path(base_dir)
        self.documents_dir = self.base_dir / documents_dir
        self.markdown_dir = self.base_dir / markdown_dir
        self.chunked_reports_dir = self.base_dir / chunked_reports_dir
        self.vector_dbs_dir = self.base_dir / vector_dbs_dir
        
        # ç¡®ä¿æ‰€æœ‰ç›®å½•å­˜åœ¨
        self.documents_dir.mkdir(parents=True, exist_ok=True)
        self.markdown_dir.mkdir(parents=True, exist_ok=True)
        self.chunked_reports_dir.mkdir(parents=True, exist_ok=True)
        self.vector_dbs_dir.mkdir(parents=True, exist_ok=True)


class RAGPipeline:
    """RAG æµç¨‹æ€»æ§ç±»"""
    
    def __init__(self, paths: Optional[PipelinePaths] = None):
        """
        åˆå§‹åŒ– RAG Pipeline
        
        Args:
            paths: è·¯å¾„é…ç½®å¯¹è±¡ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
        """
        self.paths = paths or PipelinePaths()
        self.metadata_storage = MetadataStorage(
            chunked_reports_dir=str(self.paths.chunked_reports_dir)
        )
        self.retrieval_service = RetrievalService()
        self.llm_service = LLMService()
        self.embedding_service = EmbeddingService()
        self.parser = DocumentParser()
        self.chunker = DocumentChunker()
        self.pdf_to_markdown = PDFToMarkdownService()
        self.vector_db = VectorDBService(self.embedding_service)
    
    async def answer(
        self, 
        query: str, 
        history: List[Dict[str, str]] = None,
        search_mode: int = 2,
        llm_model: int = 2,
        product_name: Optional[str] = None
    ) -> Dict:
        """
        å®Œæ•´ RAG é—®ç­”æµç¨‹ï¼ŒåŒ…å«ç»“æ„åŒ–è¾“å‡ºå’Œå¼•ç”¨éªŒè¯ (å¼‚æ­¥)
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            history: å¯¹è¯å†å²
            search_mode: æœç´¢æ¨¡å¼ï¼Œ1=çº¯å‘é‡æœç´¢ï¼Œ2=æ··åˆæ£€ç´¢+rerank
            llm_model: å¤§æ¨¡å‹é€‰æ‹©ï¼Œ1=qwen-max, 2=qwen-plus, 3=qwen-turbo
            product_name: å¯é€‰çš„äº§å“åç§°ï¼Œç”¨äºè¿‡æ»¤ç›¸å…³æ–‡æ¡£
        """
        pipeline_start = time.time()
        logger.info("[Pipeline] å¼€å§‹å¤„ç† RAG é—®ç­”æµç¨‹")
        
        # 0. è®¾ç½®LLMæ¨¡å‹
        step_start = time.time()
        self.llm_service.set_model(llm_model)
        model_name = {1: "qwen-max", 2: "qwen-plus", 3: "qwen-turbo"}.get(llm_model, "qwen-plus")
        logger.info(f"[Pipeline] æ­¥éª¤0: è®¾ç½®LLMæ¨¡å‹ -> {model_name} (è€—æ—¶: {time.time() - step_start:.2f}ç§’)")
        
        # 1. äº§å“åç§°æå–ï¼ˆå¦‚æœæœªæä¾›ï¼‰
        step_start = time.time()
        final_product_name = product_name
        if not final_product_name:
            # å°è¯•ä»æŸ¥è¯¢ä¸­æå–äº§å“åç§°ï¼ˆç®€å•å…³é”®è¯åŒ¹é…ï¼‰
            # è·å–æ‰€æœ‰æ–‡æ¡£åç§°
            all_doc_names = self.metadata_storage.get_all_document_names()
            logger.debug(f"[Pipeline] å¯ç”¨æ–‡æ¡£åç§°: {list(all_doc_names)[:5]}...")
            for doc_name in all_doc_names:
                # å¦‚æœæŸ¥è¯¢ä¸­åŒ…å«æ–‡æ¡£åç§°çš„å…³é”®è¯ï¼Œä½¿ç”¨è¯¥æ–‡æ¡£
                if doc_name in query or any(keyword in query for keyword in doc_name.split() if len(keyword) > 2):
                    final_product_name = doc_name
                    logger.info(f"[Pipeline] ä»æŸ¥è¯¢ä¸­æå–åˆ°äº§å“åç§°: {final_product_name}")
                    break
        else:
            logger.info(f"[Pipeline] ä½¿ç”¨æä¾›çš„äº§å“åç§°: {final_product_name}")
        logger.info(f"[Pipeline] æ­¥éª¤1: äº§å“åç§°æå– -> {final_product_name or 'æœªæŒ‡å®š'} (è€—æ—¶: {time.time() - step_start:.2f}ç§’)")
        
        # 2. è·å–æ–‡æ¡£SHA1ï¼ˆå¦‚æœæä¾›äº†äº§å“åç§°ï¼‰
        step_start = time.time()
        document_sha1 = None
        if final_product_name:
            document_sha1 = self.metadata_storage.get_document_sha1_by_name(final_product_name, fuzzy_match=True)
            if document_sha1:
                logger.info(f"[Pipeline] æ‰¾åˆ°æ–‡æ¡£SHA1: {document_sha1[:16]}...")
            else:
                logger.warning(f"[Pipeline] æœªæ‰¾åˆ°äº§å“åç§° '{final_product_name}' å¯¹åº”çš„æ–‡æ¡£SHA1")
        logger.info(f"[Pipeline] æ­¥éª¤2: è·å–æ–‡æ¡£SHA1 -> {document_sha1[:16] + '...' if document_sha1 else 'æ— '} (è€—æ—¶: {time.time() - step_start:.2f}ç§’)")
        
        # 3. æŸ¥è¯¢æ”¹å†™ (å¯é€‰)
        step_start = time.time()
        optimized_query = await self.llm_service.rewrite_query(query)
        logger.info(f"[Pipeline] æ­¥éª¤3: æŸ¥è¯¢æ”¹å†™")
        logger.debug(f"[Pipeline] åŸå§‹æŸ¥è¯¢: {query[:100]}...")
        logger.debug(f"[Pipeline] ä¼˜åŒ–æŸ¥è¯¢: {optimized_query[:100]}...")
        logger.info(f"[Pipeline] æ­¥éª¤3è€—æ—¶: {time.time() - step_start:.2f}ç§’")
        
        # 4. æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡
        step_start = time.time()
        logger.info(f"[Pipeline] æ­¥éª¤4: å¼€å§‹æ£€ç´¢ (æ¨¡å¼={search_mode}, top_k=10)")
        search_results = await self.retrieval_service.search(
            optimized_query,
            top_k=10,
            search_mode=search_mode,
            product_name=final_product_name,
            document_sha1=document_sha1
        )
        logger.info(f"[Pipeline] æ­¥éª¤4: æ£€ç´¢å®Œæˆï¼Œè·å¾— {len(search_results)} ä¸ªç»“æœ (è€—æ—¶: {time.time() - step_start:.2f}ç§’)")
        if search_results:
            logger.debug(f"[Pipeline] æ£€ç´¢ç»“æœç¤ºä¾‹: {search_results[0].get('document_name', 'N/A')} (ç›¸ä¼¼åº¦: {search_results[0].get('similarity', 0):.3f})")
        
        # 5. ç»„è£… Promptï¼ŒåŒ…å«ç»“æ„åŒ–è¦æ±‚
        step_start = time.time()
        context_items = []
        available_pages = set()
        for i, res in enumerate(search_results):
            page_info = f", ç¬¬ {res['page_num']} é¡µ" if res.get('page_num') else ""
            section_info = " > ".join(res.get('section_path', [])) if res.get('section_path') else ""
            section_str = f" [{section_info}]" if section_info else ""
            context_items.append(f"[{i+1}] æ¥è‡ªã€Š{res['document_name']}ã€‹{section_str}{page_info}:\n{res['text']}")
            if res.get('page_num'):
                available_pages.add(res['page_num'])
                
        context = "\n\n".join(context_items)
        logger.info(f"[Pipeline] æ­¥éª¤5: ç»„è£…ä¸Šä¸‹æ–‡ (æ€»é•¿åº¦: {len(context)} å­—ç¬¦, å¯ç”¨é¡µç : {sorted(available_pages)}) (è€—æ—¶: {time.time() - step_start:.2f}ç§’)")
        
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„éœ€æ±‚åˆ†æåŠ©æ‰‹ã€‚è¯·åŸºäºæä¾›çš„å‚è€ƒå†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
è¦æ±‚ï¼š
1. å¿…é¡»ä¸¥æ ¼åŸºäºå‚è€ƒå†…å®¹å›ç­”ï¼Œä¸è¦ç¼–é€ ã€‚
2. ç»™å‡ºæ¨ç†è¿‡ç¨‹ï¼ˆthoughtsï¼‰ã€‚
3. åˆ—å‡ºå¼•ç”¨çš„é¡µç ï¼ˆcitationsï¼‰ã€‚
4. è¾“å‡ºå¿…é¡»æ˜¯ JSON æ ¼å¼ï¼ŒåŒ…å«å­—æ®µï¼šanswer, thoughts, citationsã€‚
"""
        
        prompt = f"å‚è€ƒå†…å®¹ï¼š\n{context}\n\nç”¨æˆ·é—®é¢˜ï¼š{query}\n\nè¯·ä»¥ JSON æ ¼å¼è¾“å‡ºå›ç­”ã€‚"
        
        # 6. ç”Ÿæˆç­”æ¡ˆï¼ˆä½¿ç”¨é€‰å®šçš„æ¨¡å‹ï¼‰
        step_start = time.time()
        logger.info(f"[Pipeline] æ­¥éª¤6: è°ƒç”¨LLMç”Ÿæˆç­”æ¡ˆ (æ¨¡å‹: {model_name})")
        raw_answer = await self.llm_service.generate(prompt, system_prompt=system_prompt)
        logger.info(f"[Pipeline] æ­¥éª¤6: LLMç”Ÿæˆå®Œæˆ (ç­”æ¡ˆé•¿åº¦: {len(raw_answer)} å­—ç¬¦, è€—æ—¶: {time.time() - step_start:.2f}ç§’)")
        
        try:
            # å°è¯•è§£æ JSON
            step_start = time.time()
            clean_json = raw_answer.strip()
            if clean_json.startswith("```json"):
                clean_json = clean_json[7:-3].strip()
            elif clean_json.startswith("```"):
                clean_json = clean_json[3:-3].strip()
                
            answer_dict = json.loads(clean_json)
            logger.info(f"[Pipeline] æ­¥éª¤7: JSONè§£ææˆåŠŸ (è€—æ—¶: {time.time() - step_start:.2f}ç§’)")
            
            # 7. å¼•ç”¨éªŒè¯
            step_start = time.time()
            original_citations = answer_dict.get('citations', [])
            valid_citations = [p for p in original_citations if p in available_pages]
            answer_dict['citations'] = valid_citations
            if len(original_citations) != len(valid_citations):
                logger.warning(f"[Pipeline] å¼•ç”¨éªŒè¯: åŸå§‹å¼•ç”¨ {len(original_citations)} ä¸ªï¼Œæœ‰æ•ˆå¼•ç”¨ {len(valid_citations)} ä¸ª")
            else:
                logger.info(f"[Pipeline] æ­¥éª¤7: å¼•ç”¨éªŒè¯å®Œæˆï¼Œæœ‰æ•ˆå¼•ç”¨ {len(valid_citations)} ä¸ª (è€—æ—¶: {time.time() - step_start:.2f}ç§’)")
            
            total_time = time.time() - pipeline_start
            logger.info(f"[Pipeline] RAGæµç¨‹å…¨éƒ¨å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’")
            logger.info("-" * 80)
            
            # ç¡®ä¿thoughtsæ˜¯å­—ç¬¦ä¸²ç±»å‹
            thoughts = answer_dict.get('thoughts')
            if isinstance(thoughts, list):
                # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
                thoughts = ' '.join(str(item) for item in thoughts) if thoughts else None
            elif thoughts is not None:
                thoughts = str(thoughts)
            
            # ç¡®ä¿citationsæ˜¯æ•´æ•°åˆ—è¡¨
            citations = answer_dict.get('citations', [])
            if citations:
                # ç¡®ä¿æ‰€æœ‰å…ƒç´ éƒ½æ˜¯æ•´æ•°
                citations = [int(c) if isinstance(c, (int, str)) and str(c).isdigit() else c for c in citations]
                citations = [c for c in citations if isinstance(c, int)]
            
            return {
                "answer": str(answer_dict.get('answer', '')),
                "thoughts": thoughts,
                "citations": citations,
                "sources": search_results
            }
        except Exception as e:
            logger.error(f"[Pipeline] JSONè§£æå¤±è´¥: {str(e)}")
            logger.debug(f"[Pipeline] åŸå§‹ç­”æ¡ˆ: {raw_answer[:200]}...")
            total_time = time.time() - pipeline_start
            logger.warning(f"[Pipeline] ä½¿ç”¨åŸå§‹ç­”æ¡ˆè¿”å›ï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’")
            logger.info("-" * 80)
            return {
                "answer": raw_answer,
                "thoughts": "è§£æç»“æ„åŒ–è¾“å‡ºå¤±è´¥",
                "citations": [],
                "sources": search_results
            }

    async def ingest_directory(self, directory_path: str):
        """
        ç¦»çº¿å…¥åº“æµç¨‹ (æ”¯æŒå¼‚æ­¥)
        """
        import os
        
        all_chunks = []
        all_embeddings = []
        
        for root, dirs, files in os.walk(directory_path):
            for file in files:
                if file.endswith(('.pdf', '.docx', '.txt', '.md')):
                    file_path = os.path.join(root, file)
                    print(f"æ­£åœ¨å¤„ç†æ–‡ä»¶: {file_path}")
                    
                    try:
                        # 1. è§£æ
                        doc = self.parser.parse(file_path)
                        # 2. åˆ‡åˆ†
                        chunks = self.chunker.chunk_document(doc)
                        
                        # 3. ç”Ÿæˆ Embeddings (æ‰¹é‡å¼‚æ­¥)
                        texts = [c.text for c in chunks]
                        embeddings = await self.embedding_service.embed_documents(texts)
                        
                        # 4. æš‚å­˜
                        for chunk, emb in zip(chunks, embeddings):
                            self.metadata_storage.save_chunk(chunk)
                            all_chunks.append(chunk)
                            all_embeddings.append(emb)
                    except Exception as e:
                        print(f"å¤„ç†æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
        
        # 5. æ„å»ºå¹¶ä¿å­˜ç´¢å¼•
        chunk_ids = [c.chunk_id for c in all_chunks]
        self.retrieval_service.faiss_index.build_index(all_embeddings, chunk_ids)
        print("å…¥åº“å®Œæˆï¼")
    
    async def process_documents(
        self,
        documents_dir: Optional[str] = None,
        skip_existing: bool = True
    ):
        """
        å®Œæ•´çš„æ–‡æ¡£å¤„ç†æµç¨‹ï¼ˆå‚è€ƒ RAG-cy çš„å¤„ç†æ–¹å¼ï¼‰
        
        æµç¨‹ï¼š
        1. PDF â†’ Markdownï¼ˆä¿å­˜åˆ° debug_dataï¼‰
        2. Markdown â†’ Chunksï¼ˆä¿å­˜åˆ° metadata/chunked_reportsï¼‰
        3. Chunks â†’ Embeddings â†’ FAISSï¼ˆä¿å­˜åˆ° metadata/vector_dbsï¼‰
        
        Args:
            documents_dir: PDF æ–‡æ¡£ç›®å½•ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨ self.paths.documents_dir
            skip_existing: æ˜¯å¦è·³è¿‡å·²å¤„ç†çš„æ–‡ä»¶ï¼ˆåŸºäº SHA1 åˆ¤æ–­ï¼‰
        """
        documents_dir = Path(documents_dir) if documents_dir else self.paths.documents_dir
        
        if not documents_dir.exists():
            raise FileNotFoundError(f"æ–‡æ¡£ç›®å½•ä¸å­˜åœ¨: {documents_dir}")
        
        # è·å–æ‰€æœ‰ PDF æ–‡ä»¶
        pdf_files = list(documents_dir.glob("*.pdf"))
        
        if not pdf_files:
            print(f"åœ¨ {documents_dir} ä¸­æœªæ‰¾åˆ° PDF æ–‡ä»¶")
            return
        
        print(f"æ‰¾åˆ° {len(pdf_files)} ä¸ª PDF æ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...")
        
        processed_count = 0
        failed_count = 0
        
        for pdf_file in tqdm(pdf_files, desc="å¤„ç† PDF æ–‡æ¡£"):
            try:
                # è®¡ç®— PDF çš„ SHA1
                pdf_sha1 = calculate_file_sha1(pdf_file)
                
                # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†ï¼ˆå¦‚æœå¯ç”¨è·³è¿‡ï¼‰
                if skip_existing:
                    chunk_json_path = self.paths.chunked_reports_dir / f"{pdf_file.stem}.json"
                    faiss_path = self.paths.vector_dbs_dir / f"{pdf_sha1}.faiss"
                    
                    if chunk_json_path.exists() and faiss_path.exists():
                        print(f"è·³è¿‡å·²å¤„ç†çš„æ–‡ä»¶: {pdf_file.name}")
                        continue
                
                # æ­¥éª¤ 1: PDF â†’ Markdown
                print(f"\n[1/3] è½¬æ¢ PDF ä¸º Markdown: {pdf_file.name}")
                md_path = self.pdf_to_markdown.convert_pdf_to_markdown(
                    str(pdf_file),
                    str(self.paths.markdown_dir)
                )
                
                # æ­¥éª¤ 2: Markdown â†’ Chunks (ä¿å­˜ä¸º JSON)
                print(f"[2/3] åˆ‡åˆ† Markdown ä¸º Chunks: {pdf_file.name}")
                chunk_json_path = self.paths.chunked_reports_dir / f"{pdf_file.stem}.json"
                self.chunker.chunk_markdown_and_save(
                    md_path=str(md_path),
                    output_path=str(chunk_json_path),
                    sha1=pdf_sha1,
                    company_name=None  # å¯ä»¥åç»­ä»é…ç½®æ–‡ä»¶è¯»å–
                )
                
                # æ­¥éª¤ 3: Chunks â†’ Embeddings â†’ FAISS
                print(f"[3/3] ç”Ÿæˆå‘é‡ç´¢å¼•: {pdf_file.name}")
                await self.vector_db.process_chunk_json(
                    chunk_json_path=str(chunk_json_path),
                    output_dir=str(self.paths.vector_dbs_dir)
                )
                
                processed_count += 1
                print(f"âœ“ å®Œæˆå¤„ç†: {pdf_file.name}")
                
            except Exception as e:
                failed_count += 1
                print(f"âœ— å¤„ç†å¤±è´¥ {pdf_file.name}: {e}")
                import traceback
                traceback.print_exc()
        
        print(f"\nå¤„ç†å®Œæˆï¼æˆåŠŸ: {processed_count}, å¤±è´¥: {failed_count}")


async def process_single_pdf(
    pdf_file_path: str,
    base_dir: str = "./data",
    company_name: Optional[str] = None,
    chunk_size: int = 30,
    chunk_overlap: int = 5
):
    """
    å¤„ç†å•ä¸ª PDF æ–‡ä»¶çš„å®Œæ•´æµç¨‹
    
    æµç¨‹ï¼š
    1. PDF â†’ Markdownï¼ˆä¿å­˜åˆ° debug_dataï¼‰
    2. Markdown â†’ Chunksï¼ˆä¿å­˜åˆ° metadata/chunked_reportsï¼‰
    3. Chunks â†’ Embeddings â†’ FAISSï¼ˆä¿å­˜åˆ° metadata/vector_dbsï¼‰
    
    Args:
        pdf_file_path: PDF æ–‡ä»¶è·¯å¾„
        base_dir: åŸºç¡€æ•°æ®ç›®å½•
        company_name: å…¬å¸åç§°ï¼ˆå¯é€‰ï¼‰
        chunk_size: æ¯ä¸ª chunk çš„æœ€å¤§è¡Œæ•°
        chunk_overlap: chunk ä¹‹é—´çš„é‡å è¡Œæ•°
    """
    import asyncio
    
    print("=" * 60)
    print("æ–‡æ¡£å¤„ç†æµç¨‹")
    print("=" * 60)
    
    pdf_path = Path(pdf_file_path)
    if not pdf_path.exists():
        print(f"âŒ é”™è¯¯ï¼šPDF æ–‡ä»¶ä¸å­˜åœ¨: {pdf_file_path}")
        return
    
    print(f"ğŸ“„ PDF æ–‡ä»¶: {pdf_path.name}")
    print(f"ğŸ“ æ–‡ä»¶å¤§å°: {pdf_path.stat().st_size / 1024 / 1024:.2f} MB")
    print(f"ğŸ“‚ åŸºç¡€ç›®å½•: {base_dir}\n")
    
    # é…ç½®è·¯å¾„
    paths = PipelinePaths(
        base_dir=base_dir,
        documents_dir="documents",
        markdown_dir="debug_data",
        chunked_reports_dir="metadata/chunked_reports",
        vector_dbs_dir="metadata/vector_dbs"
    )
    
    # åˆ›å»º Pipeline å®ä¾‹
    pipeline = RAGPipeline(paths=paths)
    
    try:
        # è®¡ç®— PDF çš„ SHA1
        pdf_sha1 = calculate_file_sha1(pdf_path)
        print(f"ğŸ” SHA1: {pdf_sha1}\n")
        
        # æ­¥éª¤ 1: PDF â†’ Markdown
        print("-" * 60)
        print("æ­¥éª¤ 1/4: PDF â†’ Markdown è½¬æ¢")
        print("-" * 60)
        md_path = pipeline.pdf_to_markdown.convert_pdf_to_markdown(
            str(pdf_path),
            str(paths.markdown_dir)
        )
        print(f"âœ… Markdown æ–‡ä»¶å·²ä¿å­˜: {md_path}")
        
        # æ­¥éª¤ 2: Markdown â†’ Chunks
        print("\n" + "-" * 60)
        print("æ­¥éª¤ 2/4: Markdown â†’ Chunks åˆ‡åˆ†")
        print("-" * 60)
        chunk_json_path = paths.chunked_reports_dir / f"{pdf_path.stem}.json"
        pipeline.chunker.chunk_markdown_and_save(
            md_path=str(md_path),
            output_path=str(chunk_json_path),
            sha1=pdf_sha1,
            company_name=company_name,
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
        
        # è¯»å–å¹¶æ˜¾ç¤º chunk ç»Ÿè®¡ä¿¡æ¯
        with open(chunk_json_path, 'r', encoding='utf-8') as f:
            chunk_data = json.load(f)
        
        chunk_count = len(chunk_data['content']['chunks'])
        print(f"âœ… Chunk JSON æ–‡ä»¶å·²ä¿å­˜: {chunk_json_path}")
        print(f"   - å…±ç”Ÿæˆ {chunk_count} ä¸ª chunks")
        print(f"   - SHA1: {chunk_data['metainfo']['sha1']}")
        
        # æ­¥éª¤ 3: Chunks â†’ Embeddings
        print("\n" + "-" * 60)
        print("æ­¥éª¤ 3/4: Chunks â†’ Embeddings ç”Ÿæˆ")
        print("-" * 60)
        print(f"   æ­£åœ¨ä¸º {chunk_count} ä¸ª chunks ç”Ÿæˆ embeddings...")
        
        # æå–æ‰€æœ‰ chunk æ–‡æœ¬
        chunks = chunk_data['content']['chunks']
        texts = [chunk['text'] for chunk in chunks]
        
        embeddings = await pipeline.embedding_service.embed_documents(texts)
        print(f"âœ… æˆåŠŸç”Ÿæˆ {len(embeddings)} ä¸ª embeddings")
        print(f"   - Embedding ç»´åº¦: {len(embeddings[0])}")
        
        # æ­¥éª¤ 4: åˆ›å»ºå¹¶ä¿å­˜ FAISS ç´¢å¼•
        print("\n" + "-" * 60)
        print("æ­¥éª¤ 4/4: åˆ›å»ºå¹¶ä¿å­˜ FAISS å‘é‡ç´¢å¼•")
        print("-" * 60)
        faiss_path = await pipeline.vector_db.process_chunk_json(
            chunk_json_path=str(chunk_json_path),
            output_dir=str(paths.vector_dbs_dir)
        )
        
        print(f"âœ… FAISS ç´¢å¼•æ–‡ä»¶å·²ä¿å­˜: {faiss_path}")
        
        # éªŒè¯ç´¢å¼•æ–‡ä»¶
        faiss_file = Path(faiss_path)
        if faiss_file.exists():
            file_size = faiss_file.stat().st_size / 1024
            print(f"   - ç´¢å¼•æ–‡ä»¶å¤§å°: {file_size:.2f} KB")
        
        print("\n" + "=" * 60)
        print("âœ… æ‰€æœ‰å¤„ç†æ­¥éª¤å®Œæˆï¼")
        print("=" * 60)
        print(f"\nç”Ÿæˆçš„æ–‡ä»¶:")
        print(f"  ğŸ“ Markdown: {md_path}")
        print(f"  ğŸ“¦ Chunk JSON: {chunk_json_path}")
        print(f"  ğŸ” FAISS ç´¢å¼•: {faiss_path}")
        print(f"\nç»Ÿè®¡ä¿¡æ¯:")
        print(f"  - Chunk æ•°é‡: {chunk_count}")
        print(f"  - Embedding ç»´åº¦: {len(embeddings[0])}")
        print(f"  - SHA1: {pdf_sha1}")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    import sys
    import asyncio
    from pathlib import Path
    
    # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„ï¼Œç¡®ä¿å¯ä»¥å¯¼å…¥ app æ¨¡å—
    # è·å–å½“å‰æ–‡ä»¶çš„ç›®å½•ï¼ˆapp/services/ï¼‰
    current_file = Path(__file__).resolve()
    # è·å– backend ç›®å½•ï¼ˆé¡¹ç›®æ ¹ç›®å½•ï¼‰
    backend_dir = current_file.parent.parent.parent
    # æ·»åŠ åˆ° sys.path
    if str(backend_dir) not in sys.path:
        sys.path.insert(0, str(backend_dir))
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    base_dir = "./data"
    documents_dir = None
    skip_existing = True
    
    # è§£æå¯é€‰å‚æ•°
    i = 1
    while i < len(sys.argv):
        if sys.argv[i] == "--base-dir" and i + 1 < len(sys.argv):
            base_dir = sys.argv[i + 1]
            i += 2
        elif sys.argv[i] == "--documents-dir" and i + 1 < len(sys.argv):
            documents_dir = sys.argv[i + 1]
            i += 2
        elif sys.argv[i] == "--no-skip-existing":
            skip_existing = False
            i += 1
        elif sys.argv[i] == "--help" or sys.argv[i] == "-h":
            print("ä½¿ç”¨æ–¹æ³•:")
            print("  python -m app.services.pipeline [options]")
            print("\nå‚æ•°:")
            print("  --base-dir        åŸºç¡€æ•°æ®ç›®å½•ï¼ˆé»˜è®¤: ./dataï¼‰")
            print("  --documents-dir   PDF æ–‡æ¡£ç›®å½•ï¼ˆç›¸å¯¹äº base_dirï¼Œé»˜è®¤: documentsï¼‰")
            print("  --no-skip-existing  ä¸è·³è¿‡å·²å¤„ç†çš„æ–‡ä»¶ï¼ˆé»˜è®¤ä¼šè·³è¿‡ï¼‰")
            print("  --help, -h        æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯")
            print("\nè¯´æ˜:")
            print("  æ‰¹é‡å¤„ç†æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰ PDF æ–‡æ¡£ï¼Œæ‰§è¡Œä»¥ä¸‹æµç¨‹ï¼š")
            print("  1. PDF â†’ Markdownï¼ˆä¿å­˜åˆ° debug_dataï¼‰")
            print("  2. Markdown â†’ Chunksï¼ˆä¿å­˜åˆ° metadata/chunked_reportsï¼‰")
            print("  3. Chunks â†’ Embeddings â†’ FAISSï¼ˆä¿å­˜åˆ° metadata/vector_dbsï¼‰")
            print("\nç¤ºä¾‹:")
            print("  python -m app.services.pipeline")
            print("  python -m app.services.pipeline --base-dir ./custom_data")
            print("  python -m app.services.pipeline --documents-dir custom_docs")
            print("  python -m app.services.pipeline --no-skip-existing")
            sys.exit(0)
        else:
            print(f"âš ï¸  æœªçŸ¥å‚æ•°: {sys.argv[i]}")
            print("  ä½¿ç”¨ --help æŸ¥çœ‹å¸®åŠ©ä¿¡æ¯")
            i += 1
    
    # åˆ›å»º Pipeline å®ä¾‹
    paths = PipelinePaths(base_dir=base_dir)
    pipeline = RAGPipeline(paths=paths)
    
    # è¿è¡Œæ‰¹é‡å¤„ç†æµç¨‹
    print("=" * 60)
    print("æ‰¹é‡å¤„ç†æ–‡æ¡£æµç¨‹")
    print("=" * 60)
    print(f"ğŸ“‚ åŸºç¡€ç›®å½•: {base_dir}")
    print(f"ğŸ“ æ–‡æ¡£ç›®å½•: {documents_dir or paths.documents_dir}")
    print(f"â­ï¸  è·³è¿‡å·²å¤„ç†: {skip_existing}")
    print()
    
    try:
        asyncio.run(pipeline.process_documents(
            documents_dir=documents_dir,
            skip_existing=skip_existing
        ))
        print("\nâœ… æ‰€æœ‰æ–‡æ¡£å¤„ç†å®Œæˆï¼")
        sys.exit(0)
    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­å¤„ç†")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

